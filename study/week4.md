# What is CNN
CNN은 시각적 영상을 분석하는 데 사용되는 다층의 feedfoward(미래지향적인 해결책,feedback의 반대) 인공신경망의 한 종류이다. 
필터링 기법을 인공신경망에 적용하여 이미지를 효과적으로 처리할 수 있는 심층 신경망 기법으로 행렬로 표현된 필터의 각 요소가 데이터 처리에 
적합하도록 자동으로 학습되는 과정을 통해 이미지를 분류하는 기법이다.
합성곱 신경망은 정규화 된 버전의 다층 퍼셉트론이다. 다층 퍼셉트론은 일반적으로 완전히 연결된 네트워크, 즉 한 계층의 각 뉴런이 다음 계층의 모든 뉴런에 연결되는 신경망 구조이다.

# GoogLENet
## 등장
GoogLeNet은 2014년 이미지넷 이미지 인식 대회에서 VGGNet(VGG19)을 이기고 우승을 차지한 알고리즘이다. 
GoogLeNet은 19층의 VGG19보다 좀 더 깊은 22층으로 구성되어 있다. 
## GoogLeNet의 구조
GoogLeNet은 22개 층으로 구성되어 있다. 파란색 블럭의 층수를 세보면 22개 층임을 알 수 있다. 

## 특징
- 1 x 1 컨볼루션
![image](https://user-images.githubusercontent.com/100742454/202461189-90257f40-abc2-4964-bc59-3717b78d5832.png)

 >1 x 1 사이즈의 필터로 컨볼루션해주는 것이다. 구조도를 보면 곳곳에 1 x 1 컨볼루션 연산이 있음을 확인할 수 있다.(Convolution이란, 함수와 또 다른 함수를 반전 이동한 값을 곱한 다음  구간에 대해 적분하여 새로운 함수를 구하는 수학 연산자이다. 사용하는 이유는 이전 값과 현재 값을 연산하기 위해 사용한다.)
 1 x 1 컨볼루션은 어떤 의미를 갖는 것일까? 왜 해주는 것일까? 
 GoogLeNet에서 1 x 1 컨볼루션은 특성맵의 갯수를 줄이는 목적으로 사용된다. 특성맵의 갯수가 줄어들면 그만큼 연산량이 줄어든다.
 (feature map이란, 입력 데이터를 필터가 순회하며 계산되는 합성곱으로 구성하는 행렬)
 
 - Inception 모듈
 
![image](https://user-images.githubusercontent.com/100742454/202461092-3129404c-265c-45b8-8ef7-6abea36beac8.png)
>GoogLeNet에 실제로 사용된 모듈은 1x1 컨볼루션이 포함된 (b) 모델이다. 아까 살펴봤듯이 1x1 컨볼루션은 특성맵을 줄여주는 역할을 한다. 노란색 블럭으로 표현된 1x1 컨볼루션을 제외한 나이브(Naive 베이지안에서 나이브 는 순진하다 라는 뜻을 가지고 있습니다. 이런 수식어가 붙은 이유는 데이터셋의 모든 특징들이 동등하고 독립적이라고 가정하기 때문이다.) 버전을 살펴보면, 이전 층에서 생성된 특성맵을 1x1 컨볼루션, 3x3 컨볼루션, 5x5 컨볼루션, 3x3 최대풀링해준(컨볼루션한 feedmap의 최댓값을 뽑아 feedmap을 생성하는 과정이다.) 결과 얻은 특성맵들을 모두 함께 쌓아준다. AlexNet, VGGNet 등의 이전 CNN 모델들은 한 층에서 동일한 사이즈의 필터커널을 이용해서 컨볼루션을 해줬던 것과 차이가 있다. 따라서 좀 더 다양한 종류의 특성이 도출된다. 여기에 1x1 컨볼루션이 포함되었으니 당연히 연산량은 많이 줄어들었을 것이다. 

- global average pooling
>AlexNet, VGGNet 등에서는 fully connected (FC) 층들이 망의 후반부에 연결되어 있다. 그러나 GoogLeNet은 FC 방식 대신에 global average pooling이란 방식을 사용한다. global average pooling은 전 층에서 산출된 특성맵들을 각각 평균낸 것을 이어서 1차원 벡터를 만들어주는 것이다. 1차원 벡터를 만들어줘야 최종적으로 이미지 분류를 위한 softmax(입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수)층을 연결해줄 수 있기 때문이다.

- auxiliary classifier
>네트워크의 깊이가 깊어지면 깊어질수록 vanishing gradient 문제를 피하기 어려워진다. 그러니까 가중치를 훈련하는 과정에 역전파(back propagation)를 주로 활용하는데, 역전파과정에서 가중치를 업데이트하는데 사용되는 gradient가 점점 작아져서 0이 되어버리는 것이다. 따라서 네트워크 내의 가중치들이 제대로 훈련되지 않는다. 이 문제를 극복하기 위해서 GoogLeNet에서는 네트워크 중간에 두 개의 보조 분류기(auxiliary classifier)를 달아주었다. 


출처: [위키백과](https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EB%8C%80%EB%AC%B8) , [코딩재개발](https://bskyvision.com/539)
